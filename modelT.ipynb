{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ↓ Importing all the Pakcages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required packages are installed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import ImageFile\n",
    "\n",
    "\n",
    "print(\"All required packages are installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if GPU is being Used for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow is using GPU: True\n",
      "GPU Devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow is using GPU:\", tf.test.is_built_with_cuda())\n",
    "print(\"GPU Devices:\", tf.config.experimental.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ↓ Tensorflow Version used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf;\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Version Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)]\n",
      "Installed Libraries: ['a:\\\\PJT2\\\\envs\\\\waste_sorting', 'a:\\\\PJT2\\\\envs\\\\waste_sorting\\\\lib\\\\site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import site\n",
    "print(\"Python Version:\", sys.version)\n",
    "print(\"Installed Libraries:\", site.getsitepackages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorising TACO dataset as [Hazardous / Recyclable / Non-Recyclable / Organic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # Define source and target dataset directories\n",
    "# TACO_DATASET_DIR = r\"A:\\PJT2\\datasets\\Validation\\TACO\"\n",
    "# TARGET_DIR = r\"A:\\PJT2\\datasets\\Validation\"\n",
    "\n",
    "# CATEGORY_MAPPING = {\n",
    "#     \"Aerosol\": \"Hazardous\",\n",
    "#     \"Aluminium blister pack\": \"Hazardous\",\n",
    "#     \"Battery\": \"Hazardous\",\n",
    "#     \"Cigarette\": \"Hazardous\",\n",
    "\n",
    "#     \"Aluminium foil\": \"Recyclable\",\n",
    "#     \"Clear plastic bottle\": \"Recyclable\",\n",
    "#     \"Corrugated carton\": \"Recyclable\",\n",
    "#     \"Drink can\": \"Recyclable\",\n",
    "#     \"Drink carton\": \"Recyclable\",\n",
    "#     \"Egg carton\": \"Recyclable\",\n",
    "#     \"Food can\": \"Recyclable\",\n",
    "#     \"Glass bottle\": \"Recyclable\",\n",
    "#     \"Glass cup\": \"Recyclable\",\n",
    "#     \"Glass jar\": \"Recyclable\",\n",
    "#     \"Magazine paper\": \"Recyclable\",\n",
    "#     \"Meal carton\": \"Recyclable\",\n",
    "#     \"Metal bottle cap\": \"Recyclable\",\n",
    "#     \"Metal lid\": \"Recyclable\",\n",
    "#     \"Normal paper\": \"Recyclable\",\n",
    "#     \"Other carton\": \"Recyclable\",\n",
    "#     \"Other plastic bottle\": \"Recyclable\",\n",
    "#     \"Other plastic container\": \"Recyclable\",\n",
    "#     \"Paper bag\": \"Recyclable\",\n",
    "#     \"Paper cup\": \"Recyclable\",\n",
    "#     \"Pizza box\": \"Recyclable\",\n",
    "#     \"Plastic bottle cap\": \"Recyclable\",\n",
    "#     \"Plastic lid\": \"Recyclable\",\n",
    "#     \"Pop tab\": \"Recyclable\",\n",
    "#     \"Scrap metal\": \"Recyclable\",\n",
    "#     \"Six pack rings\": \"Recyclable\",\n",
    "#     \"Spread tub\": \"Recyclable\",\n",
    "#     \"Toilet tube\": \"Recyclable\",\n",
    "#     \"Tupperware\": \"Recyclable\",\n",
    "#     \"Wrapping paper\": \"Recyclable\",\n",
    "\n",
    "#     \"Broken glass\": \"Non-Recyclable\",\n",
    "#     \"Crisp packet\": \"Non-Recyclable\",\n",
    "#     \"Disposable food container\": \"Non-Recyclable\",\n",
    "#     \"Disposable plastic cup\": \"Non-Recyclable\",\n",
    "#     \"Foam cup\": \"Non-Recyclable\",\n",
    "#     \"Foam food container\": \"Non-Recyclable\",\n",
    "#     \"Garbage bag\": \"Non-Recyclable\",\n",
    "#     \"Other plastic\": \"Non-Recyclable\",\n",
    "#     \"Other plastic wrapper\": \"Non-Recyclable\",\n",
    "#     \"Plastic film\": \"Non-Recyclable\",\n",
    "#     \"Plastic gloves\": \"Non-Recyclable\",\n",
    "#     \"Plastic straw\": \"Non-Recyclable\",\n",
    "#     \"Plastic utensils\": \"Non-Recyclable\",\n",
    "#     \"Polypropylene bag\": \"Non-Recyclable\",\n",
    "#     \"Rope & strings\": \"Non-Recyclable\",\n",
    "#     \"Shoe\": \"Non-Recyclable\",\n",
    "#     \"Single-use carrier bag\": \"Non-Recyclable\",\n",
    "#     \"Squeezable tube\": \"Non-Recyclable\",\n",
    "#     \"Styrofoam piece\": \"Non-Recyclable\",\n",
    "#     \"Tissues\": \"Non-Recyclable\",\n",
    "#     \"Unlabeled litter\": \"Non-Recyclable\",\n",
    "\n",
    "#     \"Food waste\": \"Organic\"\n",
    "# }\n",
    "\n",
    "# # Move entire subcategory folders into the main category folders\n",
    "# for subcategory, main_category in CATEGORY_MAPPING.items():\n",
    "#     subcategory_path = os.path.join(TACO_DATASET_DIR, subcategory)\n",
    "#     target_category_path = os.path.join(TARGET_DIR, main_category)\n",
    "#     target_path = os.path.join(target_category_path, subcategory)\n",
    "\n",
    "#     if os.path.exists(subcategory_path):\n",
    "#         # Ensure main category folder exists\n",
    "#         os.makedirs(target_category_path, exist_ok=True)\n",
    "\n",
    "#         # Move the whole folder if it doesn't already exist in the target\n",
    "#         if not os.path.exists(target_path):\n",
    "#             shutil.move(subcategory_path, target_path)\n",
    "#         else:\n",
    "#             print(f\"⚠️ Folder already exists: {target_path}. Merging contents.\")\n",
    "#             for img_file in os.listdir(subcategory_path):\n",
    "#                 src = os.path.join(subcategory_path, img_file)\n",
    "#                 dst = os.path.join(target_path, img_file)\n",
    "\n",
    "#                 # Ensure unique filenames\n",
    "#                 if os.path.exists(dst):\n",
    "#                     base, ext = os.path.splitext(img_file)\n",
    "#                     counter = 1\n",
    "#                     while os.path.exists(os.path.join(target_path, f\"{base}_{counter}{ext}\")):\n",
    "#                         counter += 1\n",
    "#                     dst = os.path.join(target_path, f\"{base}_{counter}{ext}\")\n",
    "\n",
    "#                 shutil.move(src, dst)\n",
    "            \n",
    "#             # Remove the empty folder after moving files\n",
    "#             os.rmdir(subcategory_path)\n",
    "\n",
    "# print(\"✅ Dataset Restructuring Completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↓ DATA PREPROCESSING  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Paths for datasets (TACO + 4 main categories)\n",
    "# DATASET_PATH = \"A:/PJT2/datasets\"  # Root dataset folder\n",
    "# TACO_DATASET_PATH = os.path.join(DATASET_PATH, \"TACO\")  # TACO dataset\n",
    "# MAIN_CATEGORIES = [\"Hazardous\", \"Non-Recyclable\", \"Recyclable\", \"Organic\"]  # Four main categories\n",
    "\n",
    "# # Destination folders for train/val split\n",
    "# TRAIN_DIR = os.path.join(DATASET_PATH, \"Train\")\n",
    "# VAL_DIR = os.path.join(DATASET_PATH, \"Validation\")\n",
    "\n",
    "# # Ensure train/val directories exist\n",
    "# os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    "# os.makedirs(VAL_DIR, exist_ok=True)\n",
    "\n",
    "# def preprocess_main_categories():\n",
    "#     \"\"\" Process the structured dataset with 4 main waste categories. \"\"\"\n",
    "#     for category in MAIN_CATEGORIES:\n",
    "#         category_path = os.path.join(DATASET_PATH, category)\n",
    "#         if not os.path.isdir(category_path):\n",
    "#             continue\n",
    "\n",
    "#         waste_types = os.listdir(category_path)\n",
    "#         for waste_type in waste_types:\n",
    "#             waste_path = os.path.join(category_path, waste_type)\n",
    "#             if not os.path.isdir(waste_path):\n",
    "#                 continue\n",
    "\n",
    "#             images = [img for img in os.listdir(waste_path) if os.path.isfile(os.path.join(waste_path, img))]\n",
    "#             print(f\"📂 Processing {category}/{waste_type} - {len(images)} images\")\n",
    "\n",
    "#             if len(images) < 2:\n",
    "#                 print(f\"⚠️ Skipping {category}/{waste_type} (Not enough images)\")\n",
    "#                 continue\n",
    "\n",
    "#             # Split images into training (80%) and validation (20%)\n",
    "#             train_images, val_images = train_test_split(images, test_size=0.2, random_state=42)\n",
    "\n",
    "#             # Create folders inside Train & Validation\n",
    "#             train_sub_dir = os.path.join(TRAIN_DIR, category, waste_type)\n",
    "#             val_sub_dir = os.path.join(VAL_DIR, category, waste_type)\n",
    "#             os.makedirs(train_sub_dir, exist_ok=True)\n",
    "#             os.makedirs(val_sub_dir, exist_ok=True)\n",
    "\n",
    "#             # Move images\n",
    "#             for img_name in tqdm(train_images, desc=f\"🔹 Train - {category}/{waste_type}\"):\n",
    "#                 shutil.copy(os.path.join(waste_path, img_name), os.path.join(train_sub_dir, img_name))\n",
    "#             for img_name in tqdm(val_images, desc=f\"🔸 Validation - {category}/{waste_type}\"):\n",
    "#                 shutil.copy(os.path.join(waste_path, img_name), os.path.join(val_sub_dir, img_name))\n",
    "\n",
    "# def preprocess_taco_dataset():\n",
    "#     \"\"\" Process the flat 'TACO' dataset where each waste type is a folder. \"\"\"\n",
    "#     waste_types = os.listdir(TACO_DATASET_PATH)\n",
    "\n",
    "#     for waste_type in waste_types:\n",
    "#         waste_path = os.path.join(TACO_DATASET_PATH, waste_type)\n",
    "#         if not os.path.isdir(waste_path):\n",
    "#             continue\n",
    "\n",
    "#         images = [img for img in os.listdir(waste_path) if os.path.isfile(os.path.join(waste_path, img))]\n",
    "#         print(f\"📂 Processing TACO/{waste_type} - {len(images)} images\")\n",
    "\n",
    "#         if len(images) < 2:\n",
    "#             print(f\"⚠️ Skipping {waste_type} (Not enough images)\")\n",
    "#             continue\n",
    "\n",
    "#         # Split images into training (80%) and validation (20%)\n",
    "#         train_images, val_images = train_test_split(images, test_size=0.2, random_state=42)\n",
    "\n",
    "#         # Create folders inside Train & Validation\n",
    "#         train_sub_dir = os.path.join(TRAIN_DIR, \"TACO\", waste_type)\n",
    "#         val_sub_dir = os.path.join(VAL_DIR, \"TACO\", waste_type)\n",
    "#         os.makedirs(train_sub_dir, exist_ok=True)\n",
    "#         os.makedirs(val_sub_dir, exist_ok=True)\n",
    "\n",
    "#         # Move images\n",
    "#         for img_name in tqdm(train_images, desc=f\"🔹 Train - TACO/{waste_type}\"):\n",
    "#             shutil.copy(os.path.join(waste_path, img_name), os.path.join(train_sub_dir, img_name))\n",
    "#         for img_name in tqdm(val_images, desc=f\"🔸 Validation - TACO/{waste_type}\"):\n",
    "#             shutil.copy(os.path.join(waste_path, img_name), os.path.join(val_sub_dir, img_name))\n",
    "\n",
    "# # Run preprocessing for both datasets\n",
    "# preprocess_main_categories()\n",
    "# preprocess_taco_dataset()\n",
    "\n",
    "# print(\"✅ Dataset cleaning & preprocessing completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Non Image Files (.pdf / .txt / .docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import imghdr\n",
    "\n",
    "# # Define dataset directories\n",
    "# DATASET_DIRS = [\"A:/PJT2/datasets\"]\n",
    "\n",
    "# def remove_invalid_files():\n",
    "#     for dataset_path in DATASET_DIRS:\n",
    "#         for root, _, files in os.walk(dataset_path):\n",
    "#             for file in files:\n",
    "#                 file_path = os.path.join(root, file)\n",
    "#                 if imghdr.what(file_path) is None:  # Check if file is an image\n",
    "#                     print(f\"🚨 Removing non-image file: {file_path}\")\n",
    "#                     os.remove(file_path)  # Delete the invalid file\n",
    "\n",
    "# remove_invalid_files()\n",
    "# print(\"✅ Non-image files removed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import psutil\n",
    "\n",
    "# DATASET_DIRS = [\"A:/PJT2/datasets\"]\n",
    "\n",
    "# def close_file_handles():\n",
    "#     \"\"\" Tries to close file handles for locked files in the dataset directory. \"\"\"\n",
    "#     for proc in psutil.process_iter():\n",
    "#         try:\n",
    "#             for open_file in proc.open_files():\n",
    "#                 file_path = open_file.path.replace(\"\\\\\", \"/\")\n",
    "#                 for dataset_path in DATASET_DIRS:\n",
    "#                     if dataset_path in file_path:\n",
    "#                         print(f\"🔄 Closing file in use: {file_path} (Process: {proc.name()})\")\n",
    "#                         proc.terminate()\n",
    "#         except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n",
    "#             pass\n",
    "\n",
    "# close_file_handles()\n",
    "# print(\"✅ All locked files should now be accessible.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion of .webp image to .jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# def convert_images_to_jpeg():\n",
    "#     for dataset_path in DATASET_DIRS:\n",
    "#         for root, _, files in os.walk(dataset_path):\n",
    "#             for file in files:\n",
    "#                 file_path = os.path.join(root, file)\n",
    "#                 if imghdr.what(file_path) not in ['jpeg', 'png']:  # Convert only if not already in JPEG/PNG\n",
    "#                     try:\n",
    "#                         img = Image.open(file_path)\n",
    "#                         new_file_path = file_path.rsplit(\".\", 1)[0] + \".jpg\"  # Change extension\n",
    "#                         img.convert(\"RGB\").save(new_file_path, \"JPEG\")  # Convert and save as JPEG\n",
    "#                         os.remove(file_path)  # Remove old file\n",
    "#                         print(f\"🔄 Converted {file_path} → {new_file_path}\")\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"⚠️ Could not convert {file_path}: {e}\")\n",
    "\n",
    "# convert_images_to_jpeg()\n",
    "# print(\"✅ Image format conversion complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing of .gif images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_gif_files():\n",
    "#     for dataset_path in DATASET_DIRS:\n",
    "#         for root, _, files in os.walk(dataset_path):\n",
    "#             for file in files:\n",
    "#                 if file.lower().endswith(\".gif\"):\n",
    "#                     file_path = os.path.join(root, file)\n",
    "#                     print(f\"🗑️ Deleting GIF file: {file_path}\")\n",
    "#                     os.remove(file_path)\n",
    "\n",
    "# remove_gif_files()\n",
    "# print(\"✅ All GIF files have been deleted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resising of Images to 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import os\n",
    "\n",
    "# # Define dataset paths\n",
    "# TRAIN_DIR = \"A:/PJT2/datasets/Train\"\n",
    "# VAL_DIR = \"A:/PJT2/datasets/Validation\"\n",
    "\n",
    "# # Image properties\n",
    "# IMG_SIZE = (224, 224)\n",
    "# BATCH_SIZE = 32\n",
    "# AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# def preprocess_image(image, label):\n",
    "#     \"\"\" Resize and normalize image \"\"\"\n",
    "#     image = tf.image.convert_image_dtype(image, tf.float32)  # Ensure correct dtype\n",
    "#     image = tf.image.resize(image, IMG_SIZE)  # Resize to 224x224\n",
    "#     return image, label\n",
    "\n",
    "# def load_dataset(directory):\n",
    "#     \"\"\" Load dataset from directory \"\"\"\n",
    "#     dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#         directory,\n",
    "#         image_size=IMG_SIZE,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         shuffle=True,\n",
    "#         label_mode='categorical'\n",
    "#     )\n",
    "    \n",
    "#     return dataset.map(preprocess_image, num_parallel_calls=AUTOTUNE).cache().shuffle(1000).prefetch(AUTOTUNE)\n",
    "\n",
    "# # Load datasets\n",
    "# train_dataset = load_dataset(TRAIN_DIR)\n",
    "# val_dataset = load_dataset(VAL_DIR)\n",
    "\n",
    "# # Verify dataset\n",
    "# for images, labels in train_dataset.take(1):\n",
    "#     print(f\"✅ Image batch shape: {images.shape}\")\n",
    "#     print(f\"✅ Label batch shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ↓MODEL TRAINING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3054 images belonging to 4 classes.\n",
      "Found 762 images belonging to 4 classes.\n",
      "🔹 Total images in dataset folder: 3817\n",
      "✅ Training images used: 3054\n",
      "✅ Validation images used: 762\n",
      "Epoch 1/10\n",
      "  7/191 [>.............................] - ETA: 40s - loss: 1.7069 - accuracy: 0.2411"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\PJT2\\envs\\waste_sorting\\lib\\site-packages\\PIL\\Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44/191 [=====>........................] - ETA: 44s - loss: 1.5161 - accuracy: 0.3366"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\PJT2\\envs\\waste_sorting\\lib\\site-packages\\PIL\\TiffImagePlugin.py:949: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191/191 [==============================] - 92s 459ms/step - loss: 1.2476 - accuracy: 0.4398 - val_loss: 1.3269 - val_accuracy: 0.4186\n",
      "Epoch 2/10\n",
      "191/191 [==============================] - 88s 462ms/step - loss: 0.9249 - accuracy: 0.5897 - val_loss: 1.3709 - val_accuracy: 0.4462\n",
      "Epoch 3/10\n",
      "191/191 [==============================] - 90s 473ms/step - loss: 0.8190 - accuracy: 0.6568 - val_loss: 1.4492 - val_accuracy: 0.4331\n",
      "Epoch 4/10\n",
      "191/191 [==============================] - 89s 466ms/step - loss: 0.7447 - accuracy: 0.6863 - val_loss: 1.4903 - val_accuracy: 0.4528\n",
      "Epoch 5/10\n",
      "191/191 [==============================] - 88s 462ms/step - loss: 0.6802 - accuracy: 0.7210 - val_loss: 1.4849 - val_accuracy: 0.4475\n",
      "Epoch 6/10\n",
      "191/191 [==============================] - 92s 483ms/step - loss: 0.6345 - accuracy: 0.7410 - val_loss: 1.4820 - val_accuracy: 0.4593\n",
      "Epoch 7/10\n",
      "191/191 [==============================] - 89s 464ms/step - loss: 0.5931 - accuracy: 0.7688 - val_loss: 1.5104 - val_accuracy: 0.4790\n",
      "Epoch 8/10\n",
      "191/191 [==============================] - 87s 455ms/step - loss: 0.5564 - accuracy: 0.7816 - val_loss: 1.5459 - val_accuracy: 0.4711\n",
      "Epoch 9/10\n",
      "191/191 [==============================] - 88s 462ms/step - loss: 0.5156 - accuracy: 0.7993 - val_loss: 1.5985 - val_accuracy: 0.4724\n",
      "Epoch 10/10\n",
      "191/191 [==============================] - 87s 456ms/step - loss: 0.4960 - accuracy: 0.8045 - val_loss: 1.6007 - val_accuracy: 0.4869\n",
      "✅ Model saved successfully in HDF5 format!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "\n",
    "# ✅ Set Paths\n",
    "DATASET_DIR = \"A:/PJT2/datasets/Train\"\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = (224, 224)\n",
    "EPOCHS = 10\n",
    "SUBSET_SIZE = 1.0  # Set to 1.0 for full training, reduce for debugging\n",
    "\n",
    "# ✅ Create Data Generators\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    DATASET_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\", subset=\"training\"\n",
    ")\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    DATASET_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\", subset=\"validation\"\n",
    ")\n",
    "\n",
    "# ✅ Debugging: Check Image Counts\n",
    "total_train_images = len(train_generator.filenames)\n",
    "total_val_images = len(val_generator.filenames)\n",
    "total_images_in_folder = sum(len(files) for _, _, files in os.walk(DATASET_DIR))\n",
    "print(f\"🔹 Total images in dataset folder: {total_images_in_folder}\")\n",
    "print(f\"✅ Training images used: {total_train_images}\")\n",
    "print(f\"✅ Validation images used: {total_val_images}\")\n",
    "\n",
    "# ✅ Load Pretrained Model\n",
    "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # Freeze base model layers\n",
    "\n",
    "# ✅ Build Custom Model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(train_generator.num_classes, activation=\"softmax\", name=\"category_output\")(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# ✅ Compile Model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# ✅ Train Model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    steps_per_epoch=max(1, int(len(train_generator) * SUBSET_SIZE)),\n",
    "    validation_steps=max(1, int(len(val_generator) * SUBSET_SIZE)),\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "# ✅ Save Model with Fix\n",
    "try:\n",
    "    model.save(\"waste_classification_model.h5\", save_format=\"h5\")\n",
    "    print(\"✅ Model saved successfully in HDF5 format!\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ HDF5 save failed: {e}\")\n",
    "    print(\"🔄 Trying TensorFlow SavedModel format...\")\n",
    "    model.save(\"waste_classification_model\")\n",
    "    print(\"✅ Model saved in TensorFlow SavedModel format!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./Models/waste_classification_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# def predict_image(img_path, model, class_labels):\n",
    "#     img = image.load_img(\"./test_imgs/Alkaline-batteries.jpg\", target_size=(224, 224))\n",
    "#     img_array = image.img_to_array(img) / 255.0  # Normalize\n",
    "#     img_array = np.expand_dims(img_array, axis=0)  # Expand dims for model input\n",
    "#     prediction = model.predict(img_array)\n",
    "    \n",
    "#     predicted_class = class_labels[np.argmax(prediction)]\n",
    "#     confidence = np.max(prediction)\n",
    "    \n",
    "#     print(f\"Predicted Class: {predicted_class}, Confidence: {confidence:.2f}\")\n",
    "\n",
    "# # Example usage\n",
    "# class_labels = list(train_generator.class_indices.keys())\n",
    "# predict_image(\"A:/PJT2/test_imgs/000055.jpg\", model, class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loss, val_acc = model.evaluate(val_generator)\n",
    "# print(f\"Validation Accuracy: {val_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# img_path = \"./test_imgs/000055.jpg\"  # Update this with an actual image path\n",
    "# img = image.load_img(img_path, target_size=(224, 224))  # Resize to model input size\n",
    "# img_array = image.img_to_array(img) / 255.0  # Normalize\n",
    "# img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "# # Predict\n",
    "# predictions = model.predict(img_array)\n",
    "# predicted_class = np.argmax(predictions)\n",
    "\n",
    "# print(f\"Predicted Class: {predicted_class}\")  # You may need to map this to class labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Select some validation images\n",
    "# batch = next(iter(val_generator))\n",
    "# images, labels = batch\n",
    "\n",
    "# # Make predictions\n",
    "# preds = model.predict(images)\n",
    "# predicted_classes = np.argmax(preds, axis=1)\n",
    "\n",
    "# # Plot images with predicted labels\n",
    "# fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# for img, label, pred, ax in zip(images, labels, predicted_classes, axes):\n",
    "#     ax.imshow(img)\n",
    "#     ax.axis(\"off\")\n",
    "#     ax.set_title(f\"True: {np.argmax(label)}, Pred: {pred}\")\n",
    "\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
